{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6f5eb0-6d89-421f-ae7e-3b0dce4a9ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (4.9.1)\n",
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.11.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from delta-spark) (5.0.0)\n",
      "Requirement already satisfied: pyspark<3.4.0,>=3.3.0 in /usr/local/spark-3.3.1-bin-hadoop3/python (from delta-spark) (3.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.10.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.10/site-packages (from pyspark<3.4.0,>=3.3.0->delta-spark) (0.10.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml delta-spark beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58966b0b-1e7e-4d5c-a6bf-7f12bc6650c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://download.marktstammdatenregister.de/Gesamtdatenexport_20230103_22.2_5540291263d64b9997af23032250e7e8.zip\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen \n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "request = requests.get(\"https://www.marktstammdatenregister.de/MaStR/Datendownload\")\n",
    "html = request.content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "zip_url = soup.find('a', {'title': 'Download'}).get('href')\n",
    "print(zip_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947e8ca5-498d-4095-ab20-963d1e3c6bf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(135338076 bytes read, 1092148901 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m zip_data \u001b[38;5;241m=\u001b[39m urlopen(zip_url)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read zipped data to temp directory\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m zFile \u001b[38;5;241m=\u001b[39m ZipFile(BytesIO(\u001b[43mzip_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Extract zip data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m zFile\u001b[38;5;241m.\u001b[39mextractall(dataset_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:481\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:632\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(135338076 bytes read, 1092148901 more expected)"
     ]
    }
   ],
   "source": [
    "filename = (zip_url.split(\"/\")[-1] ).rsplit('.', maxsplit=1)[0]\n",
    "dataset_path = f\"/home/jovyan/{filename}\"\n",
    "# Zip url to read data\n",
    "zip_data = urlopen(zip_url)\n",
    "\n",
    "# Read zipped data to temp directory\n",
    "zFile = ZipFile(BytesIO(zip_data.read()))\n",
    "\n",
    "#Extract zip data\n",
    "zFile.extractall(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba7275-6f33-4686-9d6f-b3d97bccb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = (zip_url.split(\"/\")[-1] ).rsplit('.', maxsplit=1)[0]\n",
    "#dataset_path = f\"/home/jovyan/{filename}\"\n",
    "#print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c6963-c5a3-4aaa-8770-a5fd36c87952",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "rm -rf -r /home/jovyan/spark-warehouse/marktstammdaten_anlageneegsolar\n",
    "rm -rf -r /home/jovyan/spark-warehouse/marktstammdaten_anlageneegspeicher\n",
    "rm -rf -r /home/jovyan/spark-warehouse/marktstammdaten_anlagenstromspeicher\n",
    "rm -rf -r /home/jovyan/spark-warehouse/marktstammdaten_einheitenstromspeicher\n",
    "rm -rf -r /home/jovyan/spark-warehouse/marktstammdaten_einheitensolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd3e6f-4c8a-4390-81a9-1a248607a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for databricks\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "#from sqlalchemy import create_engine\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import zipfile\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52372706-c120-4ca4-8107-e9dfd099ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"data-gurus\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.6.1\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "#spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "\n",
    "# #Create SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[1]\").appName(\"data-gurus\").getOrCreate()\n",
    "# print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf622f2-c4c1-4b78-9de5-ac683b5f36e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark)\n",
    "print('PySpark Version :'+spark.version)\n",
    "print('PySpark Version :'+spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5b5ce-a667-4296-b085-e17c75b8cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give path of dataset and file name without the number\n",
    "def files_mask(dataset_path, filename):\n",
    "    '''\n",
    "    \n",
    "    :param dataset_path: dataset path of zip folder\n",
    "    :param filename: file name without extension type\n",
    "    :return: matching list with the same kind of files\n",
    "    '''\n",
    "    # get all files names in the datasetfolder\n",
    "\n",
    "    zf = zipfile.ZipFile(dataset_path, \"r\")\n",
    "    all_files = []\n",
    "    for file in zf.namelist():\n",
    "        if file.endswith(\".xml\"):\n",
    "            #[5:] removes DATA/ string from the beginning\n",
    "            all_files.append(file)\n",
    "\n",
    "    #all_files = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "\n",
    "    regex_pattern = \".*\" + filename + \".*\"\n",
    "\n",
    "    match_list = []\n",
    "    for i in range(0, len(all_files)):\n",
    "        match = re.findall(regex_pattern, all_files[i])\n",
    "        if not match:\n",
    "            # if list is empty\n",
    "            pass\n",
    "        else:\n",
    "            match_list.append(match[0])\n",
    "\n",
    "    # print(\"Required Files in the Dataset: \", match_list)\n",
    "    return match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f4180-c328-4019-b4c1-23d78c90dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_dataframe_converter(dataset_path, xml_file):\n",
    "    '''\n",
    "\n",
    "    :param dataset_path:zip folder path \n",
    "    :param xml_file: full file name with the extension\n",
    "    :return: pandas dataframe\n",
    "    '''\n",
    "\n",
    "    zf = zipfile.ZipFile(dataset_path, \"r\")\n",
    "\n",
    "    if xml_file in zf.namelist():\n",
    "        # print(\"xml_file > \",xml_file)\n",
    "        xml_file_open = zf.open(xml_file)\n",
    "        xml_file = xml_file_open.read()\n",
    "        # print(\"xml_file_open > \",xml_file_open)\n",
    "        \n",
    "        df = pd.read_xml(xml_file, parser=\"lxml\", encoding= \"utf-16\")\n",
    "\n",
    "    #print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e07b5c-0792-4838-83da-accb62d0410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "#this is the main function\n",
    "def xml_parser():\n",
    "    \n",
    "    # change dataset path\n",
    "    #dataset_path = \"G:\\DATA GURUS WORK\\AAAA\\\"\n",
    "    dataset_path = \"/home/jovyan/Gesamtdatenexport_20221229_22.zip\"\n",
    "   \n",
    "    file_names = ['AnlagenEegSolar', 'AnlagenEegSpeicher', 'AnlagenStromSpeicher', 'EinheitenStromSpeicher', 'EinheitenSolar']\n",
    "    #file_names = ['EinheitenStromSpeicher']\n",
    "\n",
    "    for i in range(0, len(file_names)):\n",
    "    \n",
    "        file_name = file_names[i]\n",
    "        \n",
    "        spark.sql(f\"drop table if exists default.marktstammdaten_{file_name.lower()}\")\n",
    "        #dbutils.fs.rm(f\"file:/home/jovyan/work/spark-warehouse/marktstammdaten_{file_name.lower()}\", true)\n",
    "\n",
    "        print(\"Current File Type : \", file_name)\n",
    "\n",
    "        xml_files_list = files_mask(dataset_path, file_name)\n",
    "        print(\"XML FILES LIST: \", xml_files_list)\n",
    "\n",
    "        #print(schemaDict[file_name.lower()])      #TODO: delete it\n",
    "        \n",
    "        for xml_file in xml_files_list:\n",
    "\n",
    "            print(f\"Processing : {xml_file}\")\n",
    "            data = xml_to_dataframe_converter(dataset_path,xml_file)\n",
    "            \n",
    "            #print(\"pd data frame :\", data)\n",
    "            #print(\"pd df info :\", data.info())\n",
    "            \n",
    "            if (file_name == \"EinheitenSolar\"):\n",
    "                data.Hausnummer = data.Hausnummer.astype(str)\n",
    "                data.NameStromerzeugungseinheit = data.NameStromerzeugungseinheit.astype(str)\n",
    "                data.Bundesland = data.Bundesland.astype(str)\n",
    "                data.Postleitzahl  = data.Postleitzahl .astype(str)\n",
    "                data.Gemeindeschluessel = data.Gemeindeschluessel.astype(str)\n",
    "                data.Lage = data.Lage.astype(str)\n",
    "                data.Einsatzverantwortlicher = data.Einsatzverantwortlicher.astype(str)\n",
    "                data.Einspeisungsart = data.Einspeisungsart.astype(str)\n",
    "                data.Adresszusatz = data.Adresszusatz.astype(str)\n",
    "                                \n",
    "            if (file_name == \"EinheitenStromSpeicher\"):\n",
    "                data.Technologie = data.Technologie.astype(str)\n",
    "                data.Postleitzahl = data.Postleitzahl.astype(str)                   \n",
    "                data.Einspeisungsart = data.Einspeisungsart.astype(str)                                 \n",
    "                \n",
    "            try:\n",
    "\n",
    "                #creating spark data frame   \n",
    "                sdf = spark.createDataFrame(data)\n",
    "                #sdf.printSchema()\n",
    "            except Exception as e:\n",
    "                print(\"Error while converting from pandas to spark :\", e)\n",
    "    \n",
    "            #database table name\n",
    "            table_name = file_name.lower()\n",
    "            table_name = \"marktstammdaten_\" + table_name\n",
    "\n",
    "            try:            \n",
    "                sdf.write.format(\"delta\").option(\"overwriteSchema\", \"true\").option(\"mergeSchema\", \"true\").option(\"path\", f\"file:/home/jovyan/spark-warehouse/marktstammdaten_{file_name.lower()}\").mode(\"append\").saveAsTable(f'{table_name}')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred while writing to databricks :\")\n",
    "                print(e)\n",
    "                \n",
    "    print(\"Finished processing!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f65ac19-1226-4d2e-8140-4db08de46c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255ee2b-6218-40ca-a9b1-c30dae39359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "marktstammdaten_anlageneegsolar_df = spark.sql(f\"select * from marktstammdaten_anlageneegsolar\")\n",
    "print(marktstammdaten_anlageneegsolar_df.count())\n",
    "print(marktstammdaten_anlageneegsolar_df.schema)\n",
    "#marktstammdaten_anlageneegsolar_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b9bb2-ff01-436d-b4b3-d39b40509995",
   "metadata": {},
   "outputs": [],
   "source": [
    "marktstammdaten_anlageneegspeicher_df = spark.sql(f\"select * from marktstammdaten_anlageneegspeicher\")\n",
    "print(marktstammdaten_anlageneegspeicher_df.count())\n",
    "print(marktstammdaten_anlageneegspeicher_df.schema)\n",
    "#marktstammdaten_anlageneegspeicher_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b768051-9a25-4636-8775-3e345bb13f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "marktstammdaten_anlagenstromspeicher_df = spark.sql(f\"select * from marktstammdaten_anlagenstromspeicher\")\n",
    "print(marktstammdaten_anlagenstromspeicher_df.count())\n",
    "print(marktstammdaten_anlagenstromspeicher_df.schema)\n",
    "#marktstammdaten_anlagenstromspeicher_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019131d-595a-4758-9342-6962bb2666f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "marktstammdaten_einheitenstromspeicher_df = spark.sql(f\"select * from marktstammdaten_einheitenstromspeicher\")\n",
    "print(marktstammdaten_einheitenstromspeicher_df.count())\n",
    "print(marktstammdaten_einheitenstromspeicher_df.schema)\n",
    "#marktstammdaten_einheitenstromspeicher_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fe205-b8da-4b98-afbb-d96aff3f83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "marktstammdaten_einheitensolar_df = spark.sql(f\"select * from marktstammdaten_einheitensolar\")\n",
    "print(marktstammdaten_einheitensolar_df.count())\n",
    "print(marktstammdaten_einheitensolar_df.schema)\n",
    "#marktstammdaten_einheitensolar_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bd9f1-5bf5-4e8d-b855-a17b1155ea12",
   "metadata": {},
   "source": [
    "%md\n",
    "#### Finished processing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
