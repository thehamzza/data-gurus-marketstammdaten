{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06268a5-d138-4ea5-a986-25dd3bef471e",
   "metadata": {},
   "source": [
    "**Installing unavailable requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39c8681-cb80-4169-9dec-372637655908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (4.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799b7c7-b23c-4ad9-97b5-a7ab7236af54",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d37a61b-d118-4711-9fa5-8a59731c3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for databricks\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import zipfile\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f2893-4a9b-4a57-8f5b-8e01ac8cfb93",
   "metadata": {},
   "source": [
    "**Spark Session**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52372706-c120-4ca4-8107-e9dfd099ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "\n",
    "#creating a builder that loads jar files for delta frame work\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"data-gurus\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.6.1\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "\n",
    "#old code to create session\n",
    "# #Create SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[1]\").appName(\"data-gurus\").getOrCreate()\n",
    "# print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cf622f2-c4c1-4b78-9de5-ac683b5f36e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version :3.3.1\n"
     ]
    }
   ],
   "source": [
    "#pyspark version\n",
    "print('PySpark Version :'+spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baac71a-8028-4e36-8843-c8e444238085",
   "metadata": {},
   "source": [
    "**Functions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ebed7-a8ef-4ffc-b206-644dcf47f851",
   "metadata": {},
   "source": [
    "**Masking Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f5b5ce-a667-4296-b085-e17c75b8cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give path of dataset and file name without the number\n",
    "def files_mask(dataset_path, filename):\n",
    "    '''\n",
    "    \n",
    "    :param dataset_path: dataset path of zip folder\n",
    "    :param filename: file name without extension type\n",
    "    :return: matching list with the same kind of files\n",
    "    '''\n",
    "    # get all files names in the datasetfolder\n",
    "\n",
    "    zf = zipfile.ZipFile(dataset_path, \"r\")\n",
    "    all_files = []\n",
    "    for file in zf.namelist():\n",
    "        if file.endswith(\".xml\"):\n",
    "            all_files.append(file)\n",
    "\n",
    "    regex_pattern = \".*\" + filename + \".*\"\n",
    "\n",
    "    match_list = []\n",
    "    for i in range(0, len(all_files)):\n",
    "        match = re.findall(regex_pattern, all_files[i])\n",
    "        if not match:\n",
    "            # if list is empty\n",
    "            pass\n",
    "        else:\n",
    "            match_list.append(match[0])\n",
    "\n",
    "    # print(\"Required Files in the Dataset: \", match_list)\n",
    "    return match_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebf508-8876-495a-9bc9-c15e239df3df",
   "metadata": {},
   "source": [
    "**XML Data to Pandas Dataframe Conversion Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "380f4180-c328-4019-b4c1-23d78c90dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_dataframe_converter(dataset_path, xml_file):\n",
    "    '''\n",
    "\n",
    "    :param dataset_path:zip folder path \n",
    "    :param xml_file: full file name with the extension\n",
    "    :return: pandas dataframe\n",
    "    '''\n",
    "\n",
    "    zf = zipfile.ZipFile(dataset_path, \"r\")\n",
    "\n",
    "    if xml_file in zf.namelist():\n",
    "        # print(\"xml_file > \",xml_file)\n",
    "        xml_file_open = zf.open(xml_file)\n",
    "        xml_file = xml_file_open.read()\n",
    "        # print(\"xml_file_open > \",xml_file_open)\n",
    "        \n",
    "        df = pd.read_xml(xml_file, parser=\"lxml\", encoding= \"utf-16\")\n",
    "\n",
    "    #print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b892c0-a008-4bb6-9d2d-8673afa41d67",
   "metadata": {},
   "source": [
    "**XML Parser Function, i.e Main Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e07b5c-0792-4838-83da-accb62d0410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to supress warnings by notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "#this is the main function that runs everything\n",
    "def xml_parser():\n",
    "    \n",
    "    #dataset path\n",
    "    dataset_path = \"/home/jovyan/work/DATA.zip\"\n",
    "\n",
    "    #file_names = ['AnlagenEegSolar', 'AnlagenEegSpeicher', 'AnlagenStromSpeicher', 'EinheitenStromSpeicher', 'EinheitenSolar']\n",
    "    file_names = ['EinheitenStromSpeicher']\n",
    "\n",
    "    for i in range(0, len(file_names)):\n",
    "    \n",
    "        file_name = file_names[i]\n",
    "        \n",
    "        #drops databricks table\n",
    "        spark.sql(f\"drop table if exists marktstammdaten_{file_name.lower()}\")\n",
    "\n",
    "        print(\"Current File Type : \", file_name)\n",
    "\n",
    "        xml_files_list = files_mask(dataset_path, file_name)\n",
    "        print(\"XML FILES LIST: \", xml_files_list)\n",
    "        \n",
    "        for xml_file in xml_files_list:\n",
    "\n",
    "            print(f\"Processing : {xml_file}\")\n",
    "            data = xml_to_dataframe_converter(dataset_path,xml_file)\n",
    "            \n",
    "            #problem fields\n",
    "            #converting them to strings while they are in pandas dataframe\n",
    "            if (file_name == \"EinheitenSolar\"):\n",
    "                data.Hausnummer = data.Hausnummer.astype(str)\n",
    "                data.NameStromerzeugungseinheit = data.NameStromerzeugungseinheit.astype(str)\n",
    "                data.Bundesland = data.Bundesland.astype(str)\n",
    "                data.Postleitzahl  = data.Postleitzahl .astype(str)\n",
    "                data.Gemeindeschluessel = data.Gemeindeschluessel.astype(str)\n",
    "                data.Lage = data.Lage.astype(str)\n",
    "                data.Einsatzverantwortlicher = data.Einsatzverantwortlicher.astype(str)\n",
    "                data.Einspeisungsart = data.Einspeisungsart.astype(str)\n",
    "                data.Adresszusatz = data.Adresszusatz.astype(str)\n",
    "                \n",
    "            if (file_name == \"EinheitenStromSpeicher\"):\n",
    "                data.Technologie = data.Technologie.astype(str)\n",
    "                data.Postleitzahl = data.Postleitzahl.astype(str)   \n",
    "                \n",
    "                \n",
    "            try:\n",
    "                #creating spark data frame   \n",
    "                sdf = spark.createDataFrame(data)\n",
    "                #sdf.printSchema()\n",
    "            except Exception as e:\n",
    "                print(\"Error while converting from pandas to spark :\", e)\n",
    "    \n",
    "            #database table name\n",
    "            table_name = file_name.lower()\n",
    "            table_name = \"marktstammdaten_\" + table_name\n",
    "\n",
    "            try:     \n",
    "                #\n",
    "                sdf.write.format(\"delta\").option(\"mergeSchema\", \"true\").option(\"path\", f\"file:/home/jovyan/work/spark-warehouse/marktstammdaten_{file_name.lower()}\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Exception occurred while writing to databricks :\")\n",
    "                print(e)\n",
    "                \n",
    "    print(\"Finished processing!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865ed17-69a8-4d40-b8e3-aad8d1314385",
   "metadata": {},
   "source": [
    "**Function Calls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f65ac19-1226-4d2e-8140-4db08de46c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current File Type :  EinheitenStromSpeicher\n",
      "XML FILES LIST:  ['DATA/EinheitenStromSpeicher_1.xml', 'DATA/EinheitenStromSpeicher_2.xml', 'DATA/EinheitenStromSpeicher_3.xml', 'DATA/EinheitenStromSpeicher_4.xml', 'DATA/EinheitenStromSpeicher_5.xml']\n",
      "Processing : DATA/EinheitenStromSpeicher_1.xml\n",
      "Processing : DATA/EinheitenStromSpeicher_2.xml\n",
      "Processing : DATA/EinheitenStromSpeicher_3.xml\n",
      "Processing : DATA/EinheitenStromSpeicher_4.xml\n",
      "Processing : DATA/EinheitenStromSpeicher_5.xml\n",
      "Finished processing!!!!\n"
     ]
    }
   ],
   "source": [
    "#calling main function\n",
    "xml_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959c2e1-8fb9-480d-b003-ef539cd34154",
   "metadata": {},
   "source": [
    "**TESTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bb960-ebbc-45e8-80ed-c2de06d51c58",
   "metadata": {},
   "source": [
    "Change File Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e4844a8-4bba-425d-957a-44ec9a918c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"EinheitenStromSpeicher\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0f992-3394-464a-bcb8-11302737d2bb",
   "metadata": {},
   "source": [
    "**Checks Databricks Table Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "136d0c56-ffb0-4199-9f47-4227eeb61b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(format='delta', id='0b0d165d-3905-49b0-8a7d-1aa4aba2bfb0', name='default.marktstammdaten_einheitenstromspeicher', description=None, location='file:/home/jovyan/work/spark-warehouse/marktstammdaten_einheitenstromspeicher', createdAt=datetime.datetime(2023, 1, 3, 15, 20, 38, 983000), lastModified=datetime.datetime(2023, 1, 3, 15, 26, 25, 746000), partitionColumns=[], numFiles=20, sizeInBytes=51507105, properties={}, minReaderVersion=1, minWriterVersion=2)\n"
     ]
    }
   ],
   "source": [
    "#show the format of table again, i.e first element of first column\n",
    "table = spark.sql(f\"DESCRIBE DETAIL marktstammdaten_{file_name.lower()}\").head()\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a7fb4-592c-4691-9a3f-1f5431ce9e33",
   "metadata": {},
   "source": [
    "**Table Size and Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b255ee2b-6218-40ca-a9b1-c30dae39359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count : 496499\n",
      "Schema : StructType([StructField('EinheitMastrNummer', StringType(), True), StructField('DatumLetzteAktualisierung', StringType(), True), StructField('LokationMaStRNummer', StringType(), True), StructField('NetzbetreiberpruefungStatus', LongType(), True), StructField('NetzbetreiberpruefungDatum', StringType(), True), StructField('AnlagenbetreiberMastrNummer', StringType(), True), StructField('Land', LongType(), True), StructField('Bundesland', DoubleType(), True), StructField('Landkreis', StringType(), True), StructField('Gemeinde', StringType(), True), StructField('Gemeindeschluessel', DoubleType(), True), StructField('Postleitzahl', StringType(), True), StructField('Ort', StringType(), True), StructField('Registrierungsdatum', StringType(), True), StructField('Inbetriebnahmedatum', StringType(), True), StructField('EinheitSystemstatus', LongType(), True), StructField('EinheitBetriebsstatus', LongType(), True), StructField('NichtVorhandenInMigriertenEinheiten', LongType(), True), StructField('DatumDesBetreiberwechsels', StringType(), True), StructField('DatumRegistrierungDesBetreiberwechsels', StringType(), True), StructField('NameStromerzeugungseinheit', StringType(), True), StructField('Weic_nv', LongType(), True), StructField('Kraftwerksnummer_nv', LongType(), True), StructField('Energietraeger', LongType(), True), StructField('Bruttoleistung', DoubleType(), True), StructField('Nettonennleistung', DoubleType(), True), StructField('FernsteuerbarkeitNb', DoubleType(), True), StructField('Einspeisungsart', DoubleType(), True), StructField('AcDcKoppelung', DoubleType(), True), StructField('Batterietechnologie', DoubleType(), True), StructField('Notstromaggregat', DoubleType(), True), StructField('ZugeordnenteWirkleistungWechselrichter', DoubleType(), True), StructField('SpeMastrNummer', StringType(), True), StructField('EegMaStRNummer', StringType(), True), StructField('EegAnlagentyp', LongType(), True), StructField('Technologie', StringType(), True), StructField('Einsatzort', DoubleType(), True), StructField('FernsteuerbarkeitDv', DoubleType(), True), StructField('FernsteuerbarkeitDr', DoubleType(), True), StructField('Gemarkung', StringType(), True), StructField('FlurFlurstuecknummern', StringType(), True), StructField('DatumEndgueltigeStilllegung', StringType(), True), StructField('GeplantesInbetriebnahmedatum', StringType(), True), StructField('Strasse', StringType(), True), StructField('StrasseNichtGefunden', DoubleType(), True), StructField('Hausnummer', StringType(), True), StructField('Hausnummer_nv', DoubleType(), True), StructField('HausnummerNichtGefunden', DoubleType(), True), StructField('Laengengrad', DoubleType(), True), StructField('Breitengrad', DoubleType(), True), StructField('DatumBeginnVoruebergehendeStilllegung', StringType(), True), StructField('PumpbetriebLeistungsaufnahme', DoubleType(), True), StructField('PumpbetriebKontinuierlichRegelbar', DoubleType(), True), StructField('BestandteilGrenzkraftwerk', DoubleType(), True), StructField('Adresszusatz', StringType(), True), StructField('AnschlussAnHoechstOderHochSpannung', DoubleType(), True), StructField('DatumWiederaufnahmeBetrieb', StringType(), True), StructField('GenMastrNummer', StringType(), True), StructField('Kraftwerksnummer', StringType(), True), StructField('Einsatzverantwortlicher', DoubleType(), True), StructField('Pumpspeichertechnologie', DoubleType(), True), StructField('Weic', StringType(), True), StructField('WeicDisplayName', StringType(), True), StructField('NettonennleistungDeutschland', DoubleType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#just checking the table size and schema\n",
    "aes = spark.sql(f\"select * from marktstammdaten_{file_name.lower()}\")\n",
    "print(\"Count :\", aes.count())\n",
    "print(\"Schema :\", aes.schema)\n",
    "#aes.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
