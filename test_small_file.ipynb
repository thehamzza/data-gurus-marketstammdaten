{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e37333-a0bb-4219-bb09-0589d324b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"/home/jovyan/work/persons.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42858a9b-7c3d-4d87-bc22-9a57459e5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d326ac9c-306a-4ed6-a6a8-bda28eecb4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f0ea4b58cd0>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"data-gurus\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8665666-b543-4438-a0df-cb6a8a6766af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element 'persons' at 0x7f0ea49f4b30>\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse(file_path)\n",
    "root = tree.getroot()\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8aa6141-71b7-4786-bd56-ce9455fb29e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub elem:  James <class 'str'>\n",
      "sub elem:  Smith <class 'str'>\n",
      "sub elem:  None <class 'NoneType'>\n",
      "sub elem:  1980 <class 'str'>\n",
      "sub elem:  1 <class 'str'>\n",
      "sub elem:  M <class 'str'>\n",
      "sub elem:  10000 <class 'str'>\n",
      "sub elem:  \n",
      "             <class 'str'>\n",
      "sub elem:  Michael <class 'str'>\n",
      "sub elem:  None <class 'NoneType'>\n",
      "sub elem:  Rose <class 'str'>\n",
      "sub elem:  1990 <class 'str'>\n",
      "sub elem:  6 <class 'str'>\n",
      "sub elem:  M <class 'str'>\n",
      "sub elem:  10000 <class 'str'>\n",
      "sub elem:  \n",
      "             <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "###FIRST METHOD \n",
    "### ONE DICT WITH KEYS AND VALUES IN LISTS\n",
    "data=[]\n",
    "my_dict={}\n",
    "for elem in root:\n",
    "    for sub_elem in elem:\n",
    "        # check if key already exists in the dictionary\n",
    "        if my_dict.__contains__(sub_elem.tag):\n",
    "            try:\n",
    "                my_dict[sub_elem.tag].append(sub_elem.text)\n",
    "                print(\"sub elem: \",sub_elem.text ,type(sub_elem.text))\n",
    "            except:\n",
    "                continue\n",
    "        # else create the new key and append\n",
    "        else:\n",
    "            try:\n",
    "                my_dict[sub_elem.tag] = []\n",
    "                my_dict[sub_elem.tag].append(sub_elem.text)\n",
    "                print(\"sub elem: \",sub_elem.text ,type(sub_elem.text))\n",
    "            except:\n",
    "                continue\n",
    "    #data.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c314ec2-dac0-4c76-8606-1c1920174737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'firstname': ['James', 'Michael'], 'lastname': ['Smith', None], 'middlename': [None, 'Rose'], 'dob_year': ['1980', '1990'], 'dob_month': ['1', '6'], 'gender': ['M', 'M'], 'salary': ['10000', '10000'], 'addresses': ['\\n            ', '\\n            ']}\n"
     ]
    }
   ],
   "source": [
    "print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a83f5ff-3e64-41cb-aee0-485e06305541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub elem:  James <class 'str'>\n",
      "sub elem:  Smith <class 'str'>\n",
      "sub elem:  None <class 'NoneType'>\n",
      "sub elem:  1980 <class 'str'>\n",
      "sub elem:  1 <class 'str'>\n",
      "sub elem:  M <class 'str'>\n",
      "sub elem:  10000 <class 'str'>\n",
      "sub elem:  \n",
      "             <class 'str'>\n",
      "sub elem:  Michael <class 'str'>\n",
      "sub elem:  None <class 'NoneType'>\n",
      "sub elem:  Rose <class 'str'>\n",
      "sub elem:  1990 <class 'str'>\n",
      "sub elem:  6 <class 'str'>\n",
      "sub elem:  M <class 'str'>\n",
      "sub elem:  10000 <class 'str'>\n",
      "sub elem:  \n",
      "             <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "## SECONG MENTHOD\n",
    "## LIST WITH SEPARATE DICTS \n",
    "#data list\n",
    "data=[]\n",
    "for elem in root:\n",
    "    my_dict={}\n",
    "    for sub_elem in elem:\n",
    "        try:\n",
    "            my_dict[sub_elem.tag]=(sub_elem.text)\n",
    "            print(\"sub elem: \",sub_elem.text ,type(sub_elem.text))\n",
    "        except:\n",
    "            continue\n",
    "    data.append(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf8cf3e-0afe-415e-910b-44d49143ce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'firstname': 'James', 'lastname': 'Smith', 'middlename': None, 'dob_year': '1980', 'dob_month': '1', 'gender': 'M', 'salary': '10000', 'addresses': '\\n            '}, {'firstname': 'Michael', 'lastname': None, 'middlename': 'Rose', 'dob_year': '1990', 'dob_month': '6', 'gender': 'M', 'salary': '10000', 'addresses': '\\n            '}]\n"
     ]
    }
   ],
   "source": [
    "#list of dict\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c343627a-4bab-4183-a283-76f010ecc76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, LongType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6d7c53-d7cf-42fb-9e0d-192dca232e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema dictionary\n",
    "schemaDict = {'persons': StructType([StructField('firstname', StringType(), True),\n",
    "                                 StructField('lastname', StringType(), True),\n",
    "                                 StructField('middlename', StringType(), True),\n",
    "                                 StructField('dob_year', StringType(), True),\n",
    "                                 StructField('dob_month', StringType(), True),\n",
    "                                 StructField('gender', StringType(), True),\n",
    "                                 StructField('salary', StringType(), True),\n",
    "                                StructField('addresses', StringType(), True)\n",
    "                                ])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6044d7e-a39d-4bc6-960c-4dd3bc77b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(data, schema=schemaDict['persons'])\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43a117-a0af-4458-8d01-984e9fd95831",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44719b29-febc-483e-9b3b-5d9d9ffbf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name=file_name\n",
    "sdf.write.option(\"path\", f\"file:/home/jovyan/spark-warehouse/marktstammdaten_{file_name.lower()}\").mode(\"append\").saveAsTable(table_name)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9e008-ef18-4105-873d-a6d5d74cdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "aes = spark.sql(\"select * from persons\")\n",
    "print(aes.count())\n",
    "print(aes.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837626f-b8ac-4fb4-a9c4-a9fd6fa36c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "aes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9184f863-59e0-447b-990e-17149ae7944f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [41], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9082\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#int conversion\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[43mast\u001b[49m\u001b[38;5;241m.\u001b[39mliteral_eval(a)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     b \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(a)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ast' is not defined"
     ]
    }
   ],
   "source": [
    "#Example of literal_eval\n",
    "try:\n",
    "    #string\n",
    "    a = '9082'\n",
    "    #int conversion\n",
    "    b = ast.literal_eval(a)\n",
    "except SyntaxError:\n",
    "    b = ast.literal_eval(a)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5a5a325-5b08-484e-aadc-42ec3d163c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['James', 'Smith', None, '1980', '1', 'M', '10000', '\\n            '], ['Michael', None, 'Rose', '1990', '6', 'M', '10000', '\\n            ']]\n",
      "  firstname lastname middlename dob_year dob_month gender salary  \\\n",
      "0     James    Smith       None     1980         1      M  10000   \n",
      "1   Michael     None       Rose     1990         6      M  10000   \n",
      "\n",
      "        addresses  \n",
      "0  \\n              \n",
      "1  \\n              \n"
     ]
    }
   ],
   "source": [
    "#TRYING PANDAS\n",
    "\n",
    "\n",
    "xml_data = open(file_path, 'r').read()  # Read file\n",
    "root = ET.XML(xml_data)  # Parse XML\n",
    "\n",
    "data = []\n",
    "cols = []\n",
    "for i, child in enumerate(root):\n",
    "    data.append([subchild.text for subchild in child])\n",
    "\n",
    "cols.append([subchild.tag for subchild in child])\n",
    "\n",
    "print(data)\n",
    "df = pd.DataFrame(data)  # Write in DF and transpose it\n",
    "df.columns = cols  # Update column names\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60e45e63-ed42-452d-b773-80ec5c1b070b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstname     object\n",
       "lastname      object\n",
       "middlename    object\n",
       "dob_year      object\n",
       "dob_month     object\n",
       "gender        object\n",
       "salary        object\n",
       "addresses     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd9ebf42-dab3-4ba4-86f9-4d68aa41bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   (firstname,)   object\n",
      " 1   (lastname,)    object\n",
      " 2   (middlename,)  object\n",
      " 3   (dob_year,)    object\n",
      " 4   (dob_month,)   object\n",
      " 5   (gender,)      object\n",
      " 6   (salary,)      object\n",
      " 7   (addresses,)   object\n",
      "dtypes: object(8)\n",
      "memory usage: 256.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#checking schema\n",
    "df.info(verbose = True, show_counts = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb57a5da-aa8d-4da9-8b40-a3e195de1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema dictionary\n",
    "schemaDict = {'persons': StructType([StructField('firstname', StringType(), True),\n",
    "                                 StructField('lastname', StringType(), True),\n",
    "                                 StructField('middlename', StringType(), True),\n",
    "                                 StructField('dob_year', LongType(), True),\n",
    "                                 StructField('dob_month', IntegerType(), True),\n",
    "                                 StructField('gender', StringType(), True),\n",
    "                                 StructField('salary', LongType(), True),\n",
    "                                StructField('addresses', StringType(), True)\n",
    "                                ])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e318001c-ca49-4838-920a-20d8c8534768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field dob_year: LongType() can not accept object '1980' in type <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschemaDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpersons\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m sdf\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:628\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 628\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    631\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:910\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m--> 910\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1722\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1722\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1700\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1694\u001b[0m             new_msg(\n\u001b[1;32m   1695\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of object (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) does not match with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1696\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of fields (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(obj), \u001b[38;5;28mlen\u001b[39m(verifiers))\n\u001b[1;32m   1697\u001b[0m             )\n\u001b[1;32m   1698\u001b[0m         )\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 1700\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1702\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1722\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 1722\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1645\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_long\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_long\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1644\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 1645\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9223372036854775808\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m9223372036854775807\u001b[39m:\n\u001b[1;32m   1647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject of LongType out of range, got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m obj))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1592\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 1592\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1593\u001b[0m             new_msg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m can not accept object \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m in type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (dataType, obj, \u001b[38;5;28mtype\u001b[39m(obj)))\n\u001b[1;32m   1594\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: field dob_year: LongType() can not accept object '1980' in type <class 'str'>"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(df, schema=schemaDict['persons'])\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56f48ef5-3fb0-43a7-8150-4957845cb119",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data=[['James', 'Smith', None, 1980, 1, 'M', 10000, '\\n            '], ['Michael', None, 'Rose', 1990, 6, 'M', 10000, '\\n            ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00fb7a28-a24b-40f1-9682-2db0ffff91fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [[10000001,1,0,1,'12:35','OK',10002,1,0,9,'f','NA',24,24,0,3,9,0,0,1,1,0,0,4,543],\n",
    "[10000001,2,0,1,'12:36','OK',10002,1,0,9,'f','NA',24,24,0,3,9,2,1,1,3,1,3,2,611],\n",
    "[10000002,1,0,4,'12:19','PA',10003,1,1,7,'f','NA',74,74,0,2,15,2,0,2,3,1,2,2,691]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1e248c53-2ddb-446c-8e12-743dd8d00ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<bound method NDFrame.head of       num 1  num 2  num 3  num 4   time status  num 5  num 6  num 7  num 8  \\\n",
      "0  10000001      1      0      1  12:35     OK  10002      1      0      9   \n",
      "1  10000001      2      0      1  12:36     OK  10002      1      0      9   \n",
      "2  10000002      1      0      4  12:19     PA  10003      1      1      7   \n",
      "\n",
      "   ...  num  num  num  num  num  num  num  num  num  num  \n",
      "0  ...    3    9    0    0    1    1    0    0    4  543  \n",
      "1  ...    3    9    2    1    1    3    1    3    2  611  \n",
      "2  ...    2   15    2    0    2    3    1    2    2  691  \n",
      "\n",
      "[3 rows x 25 columns]>\n"
     ]
    }
   ],
   "source": [
    "new_df=pd.DataFrame(new_data)\n",
    "print(type(new_df))\n",
    "cols=[\"num 1\",\"num 2\",\"num 3\",\"num 4\",\"time\",\"status\",\"num 5\",\"num 6\",\"num 7\",\"num 8\",\"f\", \"status\", \"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",\"num\",]\n",
    "new_df.columns=cols\n",
    "print(new_df.head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d2a428f-d5ff-4c91-af0b-97ef743442a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f79ac11-a395-431a-824f-14d1e118084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "name already used as a name or title",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn [79], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     p_schema \u001b[38;5;241m=\u001b[39m StructType(struct_list)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sqlContext\u001b[38;5;241m.\u001b[39mcreateDataFrame(pandas_df, p_schema)\n\u001b[0;32m---> 29\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_to_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [79], line 27\u001b[0m, in \u001b[0;36mpandas_to_spark\u001b[0;34m(pandas_df)\u001b[0m\n\u001b[1;32m     25\u001b[0m   struct_list\u001b[38;5;241m.\u001b[39mappend(define_structure(column, typo))\n\u001b[1;32m     26\u001b[0m p_schema \u001b[38;5;241m=\u001b[39m StructType(struct_list)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msqlContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandas_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py:471\u001b[0m, in \u001b[0;36mSQLContext.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateDataFrame\u001b[39m(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    367\u001b[0m     data: Union[RDD[Any], Iterable[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandasDataFrameLike\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m     verifySchema: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    371\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    Py4JJavaError: ...\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:436\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    434\u001b[0m             warn(msg)\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:500\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[0;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[1;32m    495\u001b[0m             pdf[column] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\n\u001b[1;32m    496\u001b[0m                 ser\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mto_pytimedelta(), index\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mindex, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mser\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    497\u001b[0m             )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# Convert pandas.DataFrame to list of numpy records\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m np_records \u001b[38;5;241m=\u001b[39m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Check if any columns need to be fixed for Spark to infer properly\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np_records) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:2549\u001b[0m, in \u001b[0;36mDataFrame.to_records\u001b[0;34m(self, index, column_dtypes, index_dtypes)\u001b[0m\n\u001b[1;32m   2546\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_mapping\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2547\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformats\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/records.py:653\u001b[0m, in \u001b[0;36mfromarrays\u001b[0;34m(arrayList, dtype, shape, formats, names, titles, aligned, byteorder)\u001b[0m\n\u001b[1;32m    650\u001b[0m     formats \u001b[38;5;241m=\u001b[39m [obj\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m arrayList]\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 653\u001b[0m     descr \u001b[38;5;241m=\u001b[39m \u001b[43msb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     descr \u001b[38;5;241m=\u001b[39m format_parser(formats, names, titles, aligned, byteorder)\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[0;31mValueError\u001b[0m: name already used as a name or title"
     ]
    }
   ],
   "source": [
    "##NEW TESTING PANDAS TO PYSPARK\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return DoubleType()\n",
    "    elif f == 'float32': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "      struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlContext.createDataFrame(pandas_df, p_schema)\n",
    "\n",
    "spark_df = pandas_to_spark(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9ad1f-1b29-4ee1-b79b-841f91fe8ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
